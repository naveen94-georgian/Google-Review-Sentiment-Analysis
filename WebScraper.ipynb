{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, argparse, traceback, threading\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "logging.basicConfig(filename='app.log', filemode='w', format='%(name)s - %(levelname)s - %(message)s')\n",
    "logging.warning('This will get logged to a file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    \"loblaw\" : \"https://www.google.co.in/maps/place/Loblaws/@44.411419,-79.709587,15z/data=!3m1!5s0x882aa2ee96e05995:0xad0170a78a2eea2c!4m7!3m6!1s0x0:0x2d502f30b7091262!8m2!3d44.411419!4d-79.709587!9m1!1b1\",\n",
    "    \"costco\" : \"https://www.google.co.in/maps/place/Costco+Wholesale/@44.3346476,-79.6836209,17z/data=!4m7!3m6!1s0x882abc4525737595:0x4c5e8a4030c6c21c!8m2!3d44.3346476!4d-79.6814322!9m1!1b1\",\n",
    "    \"walmart\" : \"https://www.google.co.in/maps/place/Walmart+Supercentre/@44.3715951,-79.7345296,13z/data=!4m10!1m2!2m1!1swalmart+barrie!3m6!1s0x882aa2eeba0d9e93:0x9bea7405337a0837!8m2!3d44.4103836!4d-79.7119763!9m1!1b1\",\n",
    "    \"zehrs\" : \"https://www.google.co.in/maps/place/Zehrs/@44.3856202,-79.7075219,13z/data=!4m10!1m2!2m1!1szehrs+barrie!3m6!1s0x882abb6242f50799:0x48515248c8d91db0!8m2!3d44.355263!4d-79.648647!9m1!1b1\",\n",
    "    \"no_frills\" : \"https://www.google.co.in/maps/place/Joe's+No+Frills/@44.3761364,-79.7052004,14.32z/data=!4m10!1m2!2m1!1swalmart+barrie!3m6!1s0x0:0x8139a99b0cc48df9!8m2!3d44.386979!4d-79.7051615!9m1!1b1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleMapReviewScrapper:\n",
    "    def __init__(self, EXE_PATH, URL, required_count):\n",
    "        '''\n",
    "        EXE_PATH - path of the chrome driver in the desktop\n",
    "        URL - url of the google maps to be scraped\n",
    "        required_count - The number of reviews to be scraped\n",
    "        '''\n",
    "        self.driver = self.__get_driver(EXE_PATH, URL)\n",
    "        self.total_count = self.get_total_count()\n",
    "        if self.total_count < required_count:\n",
    "            self.required_count = self.get_total_count()\n",
    "        else:\n",
    "            self.required_count = required_count\n",
    "        self.__sort_by_newest()\n",
    "    \n",
    "    def __enter__(self):\n",
    "        '''\n",
    "        Executed once the object is instanciated \n",
    "        '''\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, tb):\n",
    "        '''\n",
    "        Executed once the object is exited\n",
    "        '''\n",
    "        if exc_type is not None:\n",
    "            traceback.print_exception(exc_type, exc_value, tb)\n",
    "\n",
    "        self.driver.close()\n",
    "        self.driver.quit()\n",
    "\n",
    "        return True\n",
    "        \n",
    "    def __get_driver(self, EXE_PATH, URL):\n",
    "        '''\n",
    "        Create and returns a chrome driver object\n",
    "        '''\n",
    "        options = Options()\n",
    "        \n",
    "        options.add_argument(\"--window-size=1366,768\")\n",
    "\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--lang=en-GB\")\n",
    "\n",
    "        input_driver = webdriver.Chrome(EXE_PATH, options=options)\n",
    "        input_driver.get(URL)\n",
    "        \n",
    "        return input_driver\n",
    "    \n",
    "    def __wait(self, cond):\n",
    "        '''\n",
    "        __wait method waits for a part of the web page to be loaded\n",
    "        '''\n",
    "        try:\n",
    "            return WebDriverWait(self.driver, 10).until(cond)\n",
    "        except:\n",
    "            print('Element load failed')\n",
    "    \n",
    "    def __sort_by_newest(self):\n",
    "        '''\n",
    "        __sort_by_newest method sorts the reviews listed by the newest review posted\n",
    "        '''\n",
    "        wait = WebDriverWait(self.driver, 10)\n",
    "        menu_bt = self.__wait(EC.element_to_be_clickable((By.XPATH, \"//button[@data-value='Sort']\")))\n",
    "        menu_bt.click()\n",
    "        li_newest = self.__wait(EC.presence_of_element_located((By.XPATH, \"//li[@data-index='1']\")))\n",
    "        li_newest.click()\n",
    "        time.sleep(10)\n",
    "    \n",
    "    def __scroll(self):\n",
    "        '''\n",
    "        __scroll method scrolls the review list to fetch more reviews.\n",
    "        '''\n",
    "        scrollable_div = self.__wait(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.section-layout.section-scrollbox.scrollable-y.scrollable-show')))\n",
    "        self.driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "        time.sleep(4)\n",
    "    \n",
    "    def __expand_reviews(self):\n",
    "        '''\n",
    "        __expand_reviews method expands a review text div\n",
    "        '''\n",
    "        links = self.driver.find_elements_by_xpath(\"//button[@class='section-expand-review mapsConsumerUiCommonButton__blue-link']\")\n",
    "        for l in links:\n",
    "            l.click()\n",
    "        time.sleep(2)\n",
    "        \n",
    "    def get_reviews(self):\n",
    "        '''\n",
    "        get_reviews methods fetches reviews per the required count\n",
    "        '''\n",
    "        logging.warning('get_reviews')\n",
    "        while self.get_review_text_count() < self.required_count and self.get_loaded_count() < self.total_count:\n",
    "            self.__scroll()\n",
    "        self.__expand_reviews()\n",
    "        return self.parse_page_source()\n",
    "            \n",
    "        \n",
    "    def parse_page_source(self):\n",
    "        '''\n",
    "        parse_page_source method scrapes the review data from the chrome driver's page source and returns it as a \n",
    "        pandas dataframe.\n",
    "        '''\n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        contents = soup.find_all('div', {'class': 'section-review-content'})\n",
    "        reviews = []\n",
    "        for content in contents:\n",
    "            reviews.append({\n",
    "                'public_date': content.find('span', {'class': 'section-review-publish-date'}).text.strip(),\n",
    "                'user_name': content.find('div', {'class': 'section-review-title'}).text.strip(),\n",
    "                'review_text': content.find('span', {'class': 'section-review-text'}).text.strip()\n",
    "            })\n",
    "        df = pd.DataFrame(reviews) \n",
    "        df['review_text'].replace('', np.nan, inplace=True)\n",
    "        df = df[df['review_text'].notna()] \n",
    "        return df.reset_index(drop=True)\n",
    "    \n",
    "    def get_review_text_count(self):\n",
    "        '''\n",
    "        get_review_text_count returns the number of text reviews loaded into the page view.\n",
    "        '''\n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        contents = soup.find_all('span', {'class': 'section-review-text'})\n",
    "        contents_clean = [content.text.strip() for content in contents if len(content.text.strip()) > 0]\n",
    "        print('review_text_count', len(contents_clean))\n",
    "        return len(contents_clean)\n",
    "    \n",
    "    def get_total_count(self):\n",
    "        '''\n",
    "        get_total_count returns the total number of reviews in the web page.\n",
    "        '''\n",
    "        self.__wait(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.gm2-caption')))\n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        count_str = soup.find('div', {'class': 'gm2-caption'}).text.strip().split(' ')[0]\n",
    "        return int(''.join(count_str.split(',')))\n",
    "        \n",
    "    def get_loaded_count(self):\n",
    "        '''\n",
    "        get_loaded_count returns total number of loaded reviews.\n",
    "        '''\n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        count = len(soup.find_all('div', {'class': 'section-review-content'}))\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_from_url(url, name, required_count):\n",
    "    '''\n",
    "    scrape_from_url method creates an instance of the GoogleMapReviewScrapper class to scrape the review data from the url\n",
    "    and writes the scraped data into a csv file.\n",
    "    '''\n",
    "    with GoogleMapReviewScrapper('E:\\\\chromedriver\\\\chromedriver.exe', str(url), required_count) as scraper:\n",
    "        logging.warning('url '+ url)\n",
    "        start_time = time.time()\n",
    "        df = scraper.get_reviews()\n",
    "        print(name, 'size', len(df))\n",
    "        df.to_csv(name+'_scrape.csv', index=False)\n",
    "        end_time = time.time()\n",
    "        print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_text_count 8\n",
      "review_text_count 11\n",
      "review_text_count 15\n",
      "review_text_count 19\n",
      "review_text_count 22\n",
      "review_text_count 25\n",
      "review_text_count 29\n",
      "review_text_count 34\n",
      "review_text_count 38\n",
      "review_text_count 44\n",
      "review_text_count 51\n",
      "review_text_count 57\n",
      "review_text_count 62\n",
      "review_text_count 66\n",
      "review_text_count 71\n",
      "review_text_count 72\n",
      "review_text_count 78\n",
      "review_text_count 82\n",
      "review_text_count 90\n",
      "review_text_count 96\n",
      "review_text_count 100\n",
      "loblaw size 100\n",
      "163.97135066986084\n",
      "Element load failed\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2abe996379eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m '''\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mscrape_from_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-6af3ba98b805>\u001b[0m in \u001b[0;36mscrape_from_url\u001b[1;34m(url, name, required_count)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mand\u001b[0m \u001b[0mwrites\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mscraped\u001b[0m \u001b[0mdata\u001b[0m \u001b[0minto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcsv\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     '''\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mGoogleMapReviewScrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'E:\\\\chromedriver\\\\chromedriver.exe'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequired_count\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscraper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'url '\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-4695c5855f34>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, EXE_PATH, URL, required_count)\u001b[0m\n\u001b[0;32m      7\u001b[0m         '''\n\u001b[0;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEXE_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mURL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_total_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_count\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mrequired_count\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequired_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_total_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-4695c5855f34>\u001b[0m in \u001b[0;36mget_total_count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpresence_of_element_located\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCSS_SELECTOR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'div.gm2-caption'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mcount_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'gm2-caption'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Uses the url list to loop through the scrape_from_url method and scrape the data from all urls in the list and \n",
    "write the scraped data in a csv file.\n",
    "'''\n",
    "for name, url in urls.items():\n",
    "    scrape_from_url(url, name, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reads the scraped data in the csv files\n",
    "'''\n",
    "loblaws_df = pd.read_csv('scraped_data/loblaw_scrape.csv')\n",
    "no_frills_df = pd.read_csv('scraped_data/no_frills_scrape.csv')\n",
    "walmart_df = pd.read_csv('scraped_data/walmart_scrape.csv')\n",
    "costco_df = pd.read_csv('scraped_data/costco_scrape.csv')\n",
    "zehrs_df = pd.read_csv('scraped_data/zehrs_scrape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating a new column retailer in the dataframes\n",
    "'''\n",
    "loblaws_df.insert(2, \"retailer\", ['loblaws']*len(loblaws_df), True)\n",
    "no_frills_df.insert(2, \"retailer\", ['no_frills']*len(no_frills_df), True)\n",
    "walmart_df.insert(2, \"retailer\", ['walmart']*len(walmart_df), True)\n",
    "costco_df.insert(2, \"retailer\", ['costco']*len(costco_df), True)\n",
    "zehrs_df.insert(2, \"retailer\", ['zehrs']*len(zehrs_df), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Merge the data frames into a single dataframe and write the result to a csv file/ \n",
    "'''\n",
    "dfs = [loblaws_df, no_frills_df, walmart_df, costco_df, zehrs_df]\n",
    "merged_df = pd.concat(dfs)\n",
    "merged_df.to_csv('retailers_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataprog",
   "language": "python",
   "name": "dataprog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
